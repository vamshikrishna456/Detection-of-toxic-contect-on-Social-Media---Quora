{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required packages\n",
    "#Ignoring warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import operator\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import tqdm\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "tqdm_notebook().pandas()\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.preprocessing\n",
    "import keras.layers\n",
    "import keras.models\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential,Model\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Activation,  Wrapper\n",
    "from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Bidirectional, Flatten, SpatialDropout1D, LSTM\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Concatenate\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.callbacks import (EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading train data into a dataframe \n",
    "train_df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing contractions\n",
    "from pycontractions import Contractions\n",
    "cont = Contractions(api_key=\"glove-twitter-100\")\n",
    "cont.load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Function for Expanding contractions\n",
    "def contraction_removal(record):\n",
    "    global cont\n",
    "    corrections = list(cont.expand_texts([record[\"question_text\"]], precise=True))\n",
    "    record[\"question_text\"] = corrections[0]\n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Expanding contractions in train_df\n",
    "train_df = train_df.progress_apply(contraction_removal, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to correct misspelled words\n",
    "from autocorrect import Speller\n",
    "def spell_check_eng(record):\n",
    "    check = Speller(lang='en')\n",
    "    record[\"question_text\"] = check(record[\"question_text\"])\n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correcting misspelled words in train_df\n",
    "train_df = train_df.progress_apply(spell_check_eng, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing segmenter function from ekphrasis package\n",
    "from ekphrasis.classes.segmenter import Segmenter\n",
    "# segmenter using the word statistics from english Wikipedia\n",
    "seg_eng = Segmenter(corpus=\"english\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining word segmentation function\n",
    "def word_segmentation(record):\n",
    "    global seg_eng\n",
    "    record[\"question_text\"] = seg_eng.segment(record[\"question_text\"])\n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word segmentation of train_df\n",
    "train_df = train_df.progress_apply(word_segmentation, axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to remove punctuations\n",
    "def punc_remove_wrapper(record, maxlen=None):\n",
    "    puncts = [\n",
    "         ',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&',\n",
    "        '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n",
    "        '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',\n",
    "        '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '“', '★', '”',\n",
    "        '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾',\n",
    "        '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼', '⊕', '▼',\n",
    "        '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n",
    "        'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»',\n",
    "        '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø',\n",
    "        '¹', '≤', '‡', '√', '«', '»', '´', 'º', '¾', '¡', '§', '£', '₤']\n",
    "    no_punc = record[\"question_text\"]\n",
    "    for punct in puncts:   \n",
    "        if punct in record[\"question_text\"]:\n",
    "            no_punc = no_punc.replace(punct, '')\n",
    "            #print(no_punc)\n",
    "    return no_punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing punctuations from train_df \n",
    "train_df[\"question_text\"] = train_df.progress_apply(punc_remove_wrapper, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing english stop words into stop_words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing stop words from train_df\n",
    "train_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: \" \".join([w for w in str(x).lower().split() if w not in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing numbers from train_df\n",
    "train_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: re.sub(\"\\d+\",\"\",x))\n",
    "print(train_df[\"question_text\"].head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
