{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required packages\n",
    "#Ignoring warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import operator\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "\n",
    "import sklearn\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import tqdm\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "tqdm_notebook().pandas()\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.preprocessing\n",
    "import keras.layers\n",
    "import keras.models\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential,Model\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Activation,  Wrapper\n",
    "from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Bidirectional, Flatten, SpatialDropout1D, LSTM\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Concatenate\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.callbacks import (EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving the data from another notebooks\n",
    "%store -r train_df1\n",
    "%store -r test_df\n",
    "%store -r tk\n",
    "%store -r max_features\n",
    "%store -r max_len\n",
    "%store -r word_index\n",
    "%store -r train_X\n",
    "%store -r test_X\n",
    "%store -r train_y\n",
    "%store -r glove_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the embed size\n",
    "embed_size = 300\n",
    "\n",
    "# Path of the best weights model\n",
    "MODEL_PATH = \"weights_best.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras attention layer for deep neural networks\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        #print(\"init\")\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "        #print(\"build\")\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        #print(\"compute mask\")\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        #print(\"call\")\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        #print(\"compute output shape\")\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras Wrapper that implements a DropConnect Layer.\n",
    "class DropConnect(Wrapper):\n",
    "    def __init__(self, layer, prob=1., **kwargs):\n",
    "        self.prob = prob\n",
    "        self.layer = layer\n",
    "        super(DropConnect, self).__init__(layer, **kwargs)\n",
    "        if 0. < self.prob < 1.:\n",
    "            self.uses_learning_phase = True\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if not self.layer.built:\n",
    "            self.layer.build(input_shape)\n",
    "            self.layer.built = True\n",
    "        super(DropConnect, self).build()\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return self.layer.compute_output_shape(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        if 0. < self.prob < 1.:\n",
    "            self.layer.kernel = K.in_train_phase(K.dropout(self.layer.kernel, self.prob), self.layer.kernel)\n",
    "            self.layer.bias = K.in_train_phase(K.dropout(self.layer.bias, self.prob), self.layer.bias)\n",
    "        return self.layer.call(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining get_model function\n",
    "def get_model(glove_weights):\n",
    "    # Input for variable-length sequences of integers\n",
    "    input_layer = Input(shape=(max_len,))\n",
    "\n",
    "    # embedding layer\n",
    "    x = Embedding(max_features, embed_size, weights=[glove_weights], trainable=False)(input_layer)\n",
    "\n",
    "    # dropout\n",
    "    x = SpatialDropout1D(rate=0.24)(x)\n",
    "\n",
    "    # Adding two bidirectional lstm\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
    "    \n",
    "    # attention layer\n",
    "    attention = Attention(max_len)(x)\n",
    "    #attention = BatchNormalization()(attention)\n",
    "    attention = DropConnect(Dense(64, activation=\"relu\"), prob = 0.05)(attention)\n",
    "    \n",
    "    # output layer(sigmoid)\n",
    "    output_layer = Dense(1, activation=\"sigmoid\")(attention)\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    # compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining get_callbacks function\n",
    "def get_callbacks():\n",
    "    earlystopping = EarlyStopping(monitor='val_loss',\n",
    "                                  min_delta=0.0001,\n",
    "                                  patience=2,\n",
    "                                  verbose=2,\n",
    "                                  mode='auto')\n",
    "    checkpoint = ModelCheckpoint(filepath=MODEL_PATH,\n",
    "                                 monitor='val_loss',\n",
    "                                 save_best_only=True,\n",
    "                                 mode='min',\n",
    "                                 verbose=2)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                  min_lr=0.0001,\n",
    "                                  factor=0.6,\n",
    "                                  patience=1,\n",
    "                                  verbose=2)\n",
    "    tensorboard = TensorBoard(log_dir='logs1/{}'.format(time()))\n",
    "    \n",
    "    return [earlystopping, checkpoint, reduce_lr, tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_thres = []\n",
    "y_test = np.zeros((test_X.shape[0], ))        \n",
    "\n",
    "# splitting the train data into train and validation data using stratified sampling\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_X, train_y, stratify=train_y, test_size=0.2, random_state=38) \n",
    "\n",
    "# calling the get_model function\n",
    "model = get_model(glove_weights)\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "# get class weight\n",
    "weights = None\n",
    "weights = utils.class_weight.compute_class_weight('balanced', np.unique(y_train), y_train) \n",
    "\n",
    "\n",
    "# training the model\n",
    "history = model.fit(X_train, y_train, batch_size=512, epochs=5, \n",
    "          validation_data=(X_val, y_val), \n",
    "          verbose=2, class_weight=weights, callbacks=get_callbacks())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload best model\n",
    "model.load_weights(MODEL_PATH)\n",
    "\n",
    "# predicting the target class values for validation data using the trained model\n",
    "pred_y_val = model.predict([X_val], batch_size=1024, verbose=2)\n",
    "\n",
    "# calculating and displaying the f1 score at respective thresholds\n",
    "thresholds = []\n",
    "for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    res = metrics.f1_score(np.squeeze(y_val), (np.squeeze(pred_y_val)>thresh).astype(int))\n",
    "    thresholds.append([thresh, res])\n",
    "    print(\"F1 score at threshold {0} is {1}\".format(thresh, res))\n",
    "\n",
    "# displaying the best threshold    \n",
    "thresholds.sort(key=lambda x: x[1], reverse=True)\n",
    "best_thresh = thresholds[0][0]\n",
    "print(\"Best threshold: \", best_thresh)\n",
    "\n",
    "# predicting the target class values for test data using the trained model\n",
    "y_test = y_test + np.squeeze(model.predict([test_X], batch_size=1024, verbose=2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the predicted target values for test dataset into dataframe\n",
    "y_test = y_test.reshape((-1, 1))\n",
    "test_df['prediction'] = (y_test>np.mean(best_thresh)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for validation data\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_val, (pred_y_val>np.mean(best_thresh)).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for test data\n",
    "confusion_matrix(test_y, (y_test>np.mean(best_thresh)).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training history of Train and Validation Loss During model Training\n",
    "from matplotlib import pyplot\n",
    "pyplot.figure(figsize=(9,7))\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='validation')\n",
    "pyplot.title('Line Plot of Train and Validation Loss During Training With Patient Early Stopping')\n",
    "pyplot.legend()\n",
    "pyplot.xlabel('epochs')\n",
    "pyplot.ylabel('loss')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training history of Train and Validation accuracy During model Training\n",
    "pyplot.figure(figsize=(9,7))\n",
    "pyplot.plot(history.history['acc'], label='train')\n",
    "pyplot.plot(history.history['val_acc'], label='validation')\n",
    "pyplot.title('Line Plot of Train and Validation accuracy During Training With Patient Early Stopping')\n",
    "pyplot.legend()\n",
    "pyplot.xlabel('epochs')\n",
    "pyplot.ylabel('accuracy')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Records with both target and predicted value = '1'\n",
    "test_df[(test_df.target==1)&(test_df.prediction==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying the classification report \n",
    "from sklearn.metrics import classification_report\n",
    "target_names = ['Sincere', 'Insincere']\n",
    "print(classification_report(test_y, (y_test>np.mean(best_thresh)).astype(int), target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
