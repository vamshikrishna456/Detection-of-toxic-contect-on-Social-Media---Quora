{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required packages\n",
    "#Ignoring warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import operator\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import tqdm\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "tqdm_notebook().pandas()\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.preprocessing\n",
    "import keras.layers\n",
    "import keras.models\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential,Model\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Activation,  Wrapper\n",
    "from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Bidirectional, Flatten, SpatialDropout1D, LSTM\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Concatenate\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.callbacks import (EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the preprocessed train data into dataframe\n",
    "train_df = pd.read_csv('preprocessed_train_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the null values in the data\n",
    "train_df = train_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering target values into a new dataframe for splitting the data using target class values\n",
    "target_df = train_df.pop('target').to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the train data into train and test data using stratified sampling\n",
    "train_data, test_data, train_target, test_target = train_test_split(train_df, target_df, stratify=target_df, test_size=0.25, random_state=38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the target class values before and after the data splitting\n",
    "from collections import Counter\n",
    "print(f'Original train dataset shape: {Counter(target_df.target)}')\n",
    "print(f'Resampled train dataset shape: {Counter(train_target.target)}')\n",
    "print(f'Resampled test dataset shape: {Counter(test_target.target)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the train_data and train_target dataframes into train_df1 dataframe\n",
    "train_df1 = train_data.join(train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the test_data and test_target dataframes into test_df dataframe\n",
    "test_df = test_data.join(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the question text data \n",
    "words_train = train_df1['question_text']\n",
    "words_test = test_df['question_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the series dataframe to string\n",
    "words_train = words_train.to_string()\n",
    "words_test = words_test.to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the \\n and digits from the question text\n",
    "words_train = ''.join([i for i in words_train if not i.isdigit()])\n",
    "words_train = words_train.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the \\n and digits from the question text\n",
    "words_test = ''.join([i for i in words_test if not i.isdigit()])\n",
    "words_test = words_test.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the number of unique words in the train data\n",
    "len(set(words_train.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the number of unique words in the test data\n",
    "len(set(words_test.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the max no of words in the question text in train and test data\n",
    "print('Max word length of questions in train is {0:.0f}.'.format(np.max(train_df1['question_text'].apply(lambda x: len(x.split())))))\n",
    "print('Max word length of questions in test is {0:.0f}.'.format(np.max(test_df['question_text'].apply(lambda x: len(x.split())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters for performing tokenization\n",
    "max_features = 188991 #total number of unique words = unique words in both train and test datasets\n",
    "max_len = 100 # max number of words in a question to use as the max word length is around 100 for train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fill up the missing values\n",
    "train_X = train_df1[\"question_text\"].fillna(\"_na_\").values\n",
    "test_X = test_df[\"question_text\"].fillna(\"_na_\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize our training data\n",
    "tk = Tokenizer(lower = True, filters='', num_words=max_features, oov_token = True)\n",
    "tk.fit_on_texts(list(train_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our training data word index\n",
    "word_index = tk.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode training and test data sentences into sequences\n",
    "train_X = tk.texts_to_sequences(train_X)\n",
    "test_X = tk.texts_to_sequences(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the train sequece and Displaying the padded train sequence\n",
    "train_X = pad_sequences(train_X, maxlen = max_len, padding='pre', truncating='post')\n",
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the test sequence and Displaying the padded test sequence\n",
    "test_X = pad_sequences(test_X, maxlen = max_len, padding='pre', truncating='post')\n",
    "test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the target values\n",
    "train_y = train_df1['target'].values\n",
    "test_y = test_df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %store is used to use the data stored in one notebook in another notebook.\n",
    "%store train_df1\n",
    "%store test_df\n",
    "%store tk\n",
    "%store max_features\n",
    "%store max_len\n",
    "%store word_index\n",
    "%store train_X\n",
    "%store test_X\n",
    "%store train_y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
